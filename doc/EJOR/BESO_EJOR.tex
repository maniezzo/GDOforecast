%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%%
%% 
%%
%% $Id: elsarticle-template-num.tex 190 2020-11-23 11:12:32Z rishi $
%%

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}
\documentclass[review,a4paper,fleqn,3p,11pt]{elsarticle}

\usepackage{natbib}
%\usepackage[authoryear]{natbib}

\usepackage[onehalfspacing]{setspace}
%\usepackage[a4paper, total={14cm, 19cm}]{geometry}
\usepackage{url}
%\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage[algo2e,linesnumbered,ruled]{algorithm2e} 
%\usepackage{appendix}
\usepackage{tabularx}       % automatic column width
%\usepackage{subfig}         % two aligned subfigures
\usepackage{mathtools}  
\usepackage{amsfonts}       % mathbb
\usepackage{xfrac}          % slanted fractions
\usepackage{bm}             % bold greeks 
\usepackage [autostyle, english = american]{csquotes} % left and right opening quotes with "
\usepackage{wrapfig}        % text flows around image
\usepackage{array}          % Needed for custom column types
\usepackage{xcolor}
\colorlet{added}{blue!80!black} 
%\usepackage{setspace}       % for controlled interline
% \usepackage{lineno}
\usepackage{enumitem}

\usepackage{caption}
\usepackage[labelfont=sf]{subcaption}
\captionsetup{subrefformat=parens,font=footnotesize}
\subcaptionsetup[figure]{textfont=sf,position=bottom}

% Redefine X column in tabularx to center the content
\newcolumntype{C}{>{\centering\arraybackslash}X}

%% the following for reducing space around figures
\setlength{\intextsep}{4pt}
\setlength{\textfloatsep}{4pt}
\setlength{\textfloatsep}{4.0pt plus 2.0pt minus 2.0pt}
%\captionsetup{belowskip=0pt}
\setlist[itemize]{noitemsep,nolistsep}

\MakeOuterQuote{"}
\def\UrlFont{\rmfamily}

\begin{document}
\let\WriteBookmarks\relax
\def\floatpagepagefraction{1}
\def\textpagefraction{.001}

%\begin{frontmatter}

\title{Bootstrap Enhanced Scenario Optimization, a Case Study in Two-Echelon Logistics}                      

\author[1]{Livio Fenga\corref{cor1}}
\ead{L.Fenga@exeter.ac.uk}

\affiliation[1]{organization={Center for Simulation, Analytics and Modelling},
                addressline={University of Exeter Business School}, 
                city={Exeter},
%                postcode={695013}, 
%                state={Kerala},
                country={UK}}

\author[2]{Vittorio Maniezzo}
\ead{vittorio.maniezzo@unibo.it}

\affiliation[2]{organization={Department of Computer Science},
                addressline={University of Bologna}, 
%                postcode={1011 NX}, 
                city={Bologna},
                country={Italy}}

\cortext[cor1]{Corresponding author}

\begin{abstract}
We introduce the Bootstrap Enhanced Scenario Optimization (BESO) framework, a novel methodology for prescriptive analytics under uncertainty. BESO addresses the challenge of generating reliable demand scenarios from volatile, short, and non-stationary time series data. The core innovation lies in delegating the modeling of uncertainties to the Maximum Entropy Bootstrap (MEB) integrated with bagging, a technique that preserves the complex autocorrelation structure of empirical time series forecasts, enabling the generation of statistically coherent scenarios. We further extend MEB through an objective-augmented formulation that incorporates downstream recourse elements into the resampling mechanism. These statistically rich scenarios are then integrated into a deterministic equivalent model to derive accountable and optimized prescriptions. We demonstrate the framework's power using a real-world case study in tactical inventory allocation. Our findings, also validated against an extensive set of artificial benchmark instances, demonstrate the effectiveness of MEB-enhanced forecasting in optimization. BESO establishes a new, generalizable paradigm for robust decision-making derived from noisy univariate data series.
\end{abstract}

%%Graphical abstract (commented out for review)
\begin{graphicalabstract}
\includegraphics[width=\linewidth]{Visual-abstract}
\end{graphicalabstract}

%%Research highlights (commented out for review)
\begin{highlights}
	\item A new framework for integrating time series forecasting into stochastic programming.
	\item Maximum Entropy Bootstrap with bagging for forecasting short logistic time series.
	\item Bootstrap distributions for distributionally robust optimization.
	\item Predictive analytics for client allocation and warehouse sizing.
	\item Real-world 2-echelon logistics case study.
\end{highlights}

\begin{keyword}
	Distributionally Robust Optimization \sep
	Stochastic programming \sep
	Forecasting \sep
	Two-echelon logistics \sep
	Maximum Entropy Bootstrap \sep 
	Short time series
\end{keyword}

\maketitle

%% main text
\section{Introduction} \label{Sec:intro}

Decision-making under uncertainty is a pervasive challenge in operations research, particularly in complex systems such as logistics networks. Scenario-based optimization has long been used to address this challenge by capturing the variability of future events through a finite set of possible outcomes. However, the reliability of such methods depends heavily on how accurately these scenarios represent the underlying stochastic processes.

In this paper, we propose a novel method — Bootstrap Enhanced Scenario Optimization (BESO) — that integrates bootstrap-based forecasting with scenario-based linear optimization to improve the robustness and realism of stochastic modeling. This work can be framed within the Distributionally Robust Optimization (DRO) area, but unlike mainstream moment- or ambiguity-set-based approaches, it leverages statistical models of the available empirical data to construct scenarios. In addition, BESO introduces a decision-aware extension of the Maximum Entropy Bootstrap (MEB), allowing the bootstrap mechanism to incorporate objective-related elements so that the generated scenarios remain statistically coherent while becoming more aligned with optimization-relevant patterns. Its effective application is proposed for predictive contexts where reliable forecasts of historical time series are available.

The resulting data-driven methodology for empirically estimating the distribution of stochastic variables and embedding it in scenario generation for robust and stochastic optimization may offer significant advantages over standard distribution-aware approaches. Traditional methods often rely on strong assumptions about the underlying probability distribution — typically normality or other parametric forms — which may not accurately reflect the true behavior of real-world uncertainties. In contrast, data-driven techniques directly leverage historical or observed data to construct empirical distributions, capturing nuances such as asymmetry, multimodality, or heavy tails that standard models might overlook. This increased fidelity can lead to more realistic and effective scenario generation, improving both the robustness and relevance of the resulting optimization solutions. Furthermore, by adapting to the structure present in the data, data-driven approaches are naturally more flexible and can better accommodate nonstationarities or structural changes in the stochastic environment, providing better alignment with real operational environments.

%The BESO framework leverages bootstrapped time series scenarios, generated through the enhanced MEB mechanism, which are then embedded within a constrained stochastic linear optimization problem, in which an objective function is optimized, possibly subject to robust, stochastic constraints. Unlike traditional stochastic programming methods that may rely on assumptions of distributional form or limited scenario diversity, BESO incorporates the empirical structure and seasonality of historical data to generate richer and more representative scenario sets.

To demonstrate the practical value and performance of BESO, we apply it to a real-world two-echelon logistics network problem. In this setting, uncertain outbound freight demands must be satisfied across multiple depots. By using BESO to define demand scenarios and solve for optimal allocation decisions and related inventory requests, we show how the method improves the solution robustness compared to conventional techniques, while remaining conceptually transparent, computationally efficient and straightforward to implement.

The remainder of the paper is structured as follows: Section \ref{Sec:literature} reviews previous works related to the different research areas intersected by this study. Section \ref{Sec:testcase} describes the case study that motivated our research. Section \ref{Sec:forecasting} and \ref{Sec:optimization} present the BESO methodology in detail.  Section \ref{Sec:results} discusses the computational results obtained, and Section \ref{Sec:conclusions} concludes with key insights and future research directions.

% The test case deals with one of the subproblems that arise when optimizing a 2-echelon logistics chain. In fact, three subproblems are typically identified, namely {\it Facility Location Problem} (FLP) \citep{DH04}, which asks to determine where to locate hubs or distribution centers and which is mainly relevant at the strategic level, Allocation Problem (AP) \citep{KSI13}, which asks to determine which center will service each client, and which is mainly relevant at the tactical level, and Vehicle Routing Problem (VRP) \citep{TV14}, which is about how to define the service routes once the customers of each center have been identified, which is mainly relevant at the operational level. 

% The proposed forecasting module, contrary to the previous work, is now based on a bootstrap scheme suitable for dependent data (see, e.g., \citep{kreiss2011bootstrap}), a well-known but often neglected methodology that exhibits interesting properties from a managerial perspective and whose comparative effectiveness is also empirically demonstrated by the results we report. Our methodology uses univariate time series forecasting with Maximum Entropy Bootstrap (MEB) and bagging to predict demand, effectively addressing challenges posed by short, non-stationary time series data.

% Univariate time series forecasting focuses on predicting future values of a single variable, which is crucial for industries relying on accurate demand predictions. The MEB method generates resampled datasets that capture the probabilistic structure of the original time series, maintaining its inherent uncertainties, particularly when data is limited.

% Bagging, or bootstrap aggregating, enhances predictive performance by training multiple models on bootstrapped samples and aggregating their predictions, helping to reduce overfitting. This approach also effectively handles the non-stationarity of time series data, allowing us to adapt to changing patterns and produce more reliable demand forecasts.

\section{Literture review} \label{Sec:literature}

The paper intersects different research threads, both in terms of the theoretical contribution and the application use case.

\subsection{Predictive Prescriptions and Distributionally Robust Optimization}

Stochastic programming has long served as a fundamental framework for modeling decision-making under uncertainty, primarily through expected-value models and recourse formulations \citep{B55,D55}. As the field matured, methods evolved to address model robustness. Robust optimization \citep{BN98,BS04} proposes a deterministic reformulation considering worst-case outcomes within a defined uncertainty set. A major development is {\it Distributionally Robust Optimization} (DRO), which seeks a solution most resilient against the worst-case probability distribution for uncertain parameters, typically within an {\it ambiguity set} \citep{DY10}.

The ambiguity set ($\mathcal{P}$) in DRO is a carefully constructed set of probability distributions that the true, but unknown, distribution of the uncertain parameters is assumed to belong to. The core of DRO is to optimize the worst-case performance over all distributions within this set, thus making the resulting solution robust to distributional uncertainty. Mainstream DRO approaches define this ambiguity set in different ways. Some utilize specific distance metrics to construct an ambiguity set centered around the empirical distribution, such as those based on the $\phi$-divergence (or $f$-divergence) \citep{BK19}. Others, such as in the work of \citet{EK17}, define the set based on measurable moment constraints (e.g., bounds on the mean and covariance) of the uncertain parameters. This methodology provides strong theoretical guarantees but requires the designer to make choices on the ambiguity set's structure and size, which may not fully reflect the true statistical dynamics of the input data.

A primary technique for applying both stochastic programming and DRO is {\it Scenario-Based Optimization}. This approach discretizes the continuous probability distribution of uncertain parameters into a finite set of specific scenarios. The {\it Sample Average Approximation} (SAA) method is a pivotal technique here \citep{SH98}, approximating the true underlying distribution with an empirical distribution derived from a sample of independent and identically distributed realizations.

The integration of forecasting and scenario-based optimization—often termed {\it predict-then-optimize} —is a growing body of literature \citep{R18,FTW18,BK19}. The typical workflow follows a two-stage paradigm: a forecasting model (e.g., SARIMA, GARCH, LSTM) first estimates future parameters, and a separate scenario generation phase simulates or samples future paths based on the forecast's predictive distribution. This sequential paradigm has been questioned because prediction errors, even small ones, can lead to suboptimal decisions when their impact on the downstream optimization problem is large. This motivated the development of methods that explicitly incorporate the optimization objective into the forecasting model's loss function \citep{DAK17,WDT19}, leading to the "Smart Predict, then Optimize" (SPO) framework \citep{EG22}, besides the aforementioned DRO contributions.

Unlike these traditional DRO and predict-then-optimize approaches, our work introduces a novel framework that fundamentally links the statistical modeling of input data to the generation of the uncertainty set. Our {\it Bootstrap Enhanced Scenario Optimization} (BESO) framework leverages bootstrapped time series to generate future scenarios. This approach is conceptually aligned with DRO, as it seeks robustness across a set of statistically plausible distributions. However, by using the Maximum Entropy Bootstrap (MEB), BESO generates scenarios that directly preserve the complex autocorrelation and non-stationarity observed in the historical data, eliminating the need for post-hoc parametric ambiguity set design. Scenarios are not a post-processing step but emerge directly from a statistically robust model of the input data's empirical structure. Furthermore, we show how the MEB objective function can be extended to incorporate the downstream optimization's recourse objective, thereby aligning the statistical model directly with the desired prescriptive outcome, similar to SPO methods.

\subsection{2-echelon logistics networks}

Multi-echelon inventory management is a supply chain control strategy that coordinates inventory across multiple levels, called "echelons," of the chain \citep{Clark60}. The objective is to optimize inventory and routing costs across all levels of the supply chain rather than addressing each level in isolation. The literature on this strategy has expanded significantly, covering various aspects such as inventory control policies, replenishment strategies, and inventory risk management.

The successful implementation of effective management policies hinges on sound forecasting and optimization processes. Specifically, in the context of 2-echelon logistics, prescriptions grounded in predictive analytics are essential for refining demand forecasting, inventory allocation, and transportation planning \citep{KK10,A01}. Considerable attention has been paid to demand forecasting \citep{Shenstone01,Syntetos05,Ma15}, however, precise demand forecasting, especially within the context of dynamic retail environments characterized by fluctuating trends and seasonal variations, remains a significant obstacle. Multi-echelon demand forecasting commonly incorporates a variety of models that frequently rely on assumptions of stationarity and specific parametric distributional characteristics. These assumptions are often violated in dynamic retail environments characterized by short-horizon, non-stationary demand series, leading to model mismatch and suboptimal prescriptions.

The application of prescriptive analytics in 2-echelon logistics systems thus faces a critical challenge: how to robustly model the underlying uncertainty to support efficient management decision processes. Most documented multi-echelon inventory policies in the literature rely on instances with generic demand, often generated according to parametric distributions \citep{Escorcia20,EG81}. Our work addresses this methodological gap.

By introducing the BESO framework, we provide a statistically-grounded approach to tactical logistics planning. BESO leverages the MEB to generate demand scenarios that accurately preserve the complex autocorrelation and non-stationary structure of empirical demand data. This novel method directly counters the limitations of traditional approaches that fail to capture the nuances of real-world volatility, enabling the distribution centers (DCs) to update their market forecast and anticipate the structural attributes of the model, ultimately providing valuable management insights \citep{P16}. We demonstrate this capability using a comprehensive real-world case study where all instance elements, from demand series to network configurations, are derived from actual records. This application provides a validation of how BESO improves both solution robustness and operational efficiency in multi-echelon inventory management.

% For example, \citep{A01} provides insights into how predictive analytics and collaborative forecasting can improve the performance of multi-echelon supply chains by anticipating demand and aligning inventory levels across different echelons, while \citep{VAD12} discusses the application of predictive analytics for inventory management in multi-echelon logistics systems, focusing on SKU classification and demand forecasting to optimize inventory levels across different echelons. Other examples can be found in \citep{R97,P16,BS17}. Predictive analytics intersects with big data and Industry 4.0, enabling logistics companies to anticipate trends, manage risk, and make informed decisions. \citep{WGNP16,HR17}. 

%Bootstrap methods can enhance the performance of various machine learning models, including Multilayer Perceptrons (MLP) and Long Short-Term Memory (LSTM) networks. While these methods generally do not require strong distributional assumptions, they can benefit from the application of bootstrap techniques, as these can improve model robustness and mitigate overfitting, addressing concerns that are especially prevalent in scenarios with limited data availability. Studies demonstrate that integrating bootstrap methods with MLP and LSTM significantly improves predictive accuracy \citep{efron1994introduction,hastie2009elements,cho2014learning}. 

% Turning to optimization of two-echelon logistics systems, we see again that a large number of mathematical models have been proposed \citep{Gumus10,Dai17,TGM23}. The primary concern revolves around demand, which is often assumed to be stochastic or specified as stationary. The goal of the optimization process is typically to develop a mathematical model that quantifies the total system cost, possibly enabling the determination of the optimal lot size to be ordered and the reorder point at each level of the supply chain

%A  model proposed in \citep{Axsater00} deals with a two-level inventory system that integrates a central warehouse with a number of different retailers, whose  demand was generated by a compound Poisson demand, analogously \citep{Andersson00} studied a two-level inventory system model with a central warehouse and multiple non-identical retailers.
%A multistage supply chain model based on Autoregressive Integrated Moving Average (ARIMA) time-series models has been presented in \citep{gilbert2005arima}. It establishes that the orders and inventories at each stage are also ARIMA processes, given an ARIMA model of consumer demand and associated lead times.
%Recent developments introduce heuristic approaches for cost optimization, with seasonal or cyclical demands  \citep{Sakulsom19}, or multi-objective optimization and vehicle synchronization \citep{ANHC21}.

% The increasing complexity of large-scale retail trade (LSRT) supply chains has led to an increase in the need for efficient logistics, and this is particularly true in the area of multi-echelon distribution systems. A 2-echelon logistics network, characterized by two levels of supply chain entities, plays a critical role in ensuring the timely and cost-effective delivery of goods. It is a complex system involving interdependent factors such as inventory management, transportation costs, demand variability, and service level requirements. Optimizing these networks is critical for companies seeking to minimize costs while maintaining high service levels, and the study of multi-echelon supply chains with periodic review ordering policies has long prompted specific lines of research \citep{A01,Clark60}. 

% In recent years, {\it predictive analytics} \cite{S10,Si13} and {\it prescriptive analytics} \cite{BK19,PH15} have emerged as powerful tools for improving decision making in various domains, including logistics \citep{DJS18}. By leveraging historical data and forecasting modules, possibly including machine learning and statistical models, prescriptive analytics enables organizations to anticipate trends, identify potential risks, and support informed decisions \citep{HR17,KS17}. It is therefore one of the primary tools to be considered for optimizing the supply chain. 

% Flores et al. (2014), "A Scenario-based Stochastic Model for Supply Chain Planning under Demand Uncertainty"
% Mula et al. (2006), "Mathematical programming models for supply chain production and transport planning"

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The 2-echelon test case} \label{Sec:testcase}

The test case, that makes use of data proposed in \citet{MZ23} for a different application, comes from an LSRT company that manages multiple retail stores in a large city in northern Italy. Among these, 52 stores provided aggregate sales time series data spanning at least 45 months prior to the forecast period, which is set at a three-month horizon, as detailed in the following. Each store maintains a small inventory, replenished by deliveries from one of four different distribution centers (DCs) in the area. The products are categorized into three lines: fresh, dry, and household. In this context, the focus has been exclusively on the logistics of dry and household goods, including product categories ranging from personal care items to basic electronics, from household goods to holiday decorations. For this product line, DCs are required to maintain an aggregate inventory sufficient to cover a demand for a period of two weeks.

The supply chain consists of a large distribution center (DC) in the northern suburbs of the city, a smaller DC in the south, and two small satellite facilities closer to the city center. The satellites have limited storage capacity, but they offer lower-cost and more timely service to the majority of the city's stores. 
%Figure \ref{fig:locations}:a shows the (anonymized) geolocation of the DCs and stores and \ref{fig:locations}:b the GIS model of the locations (see also \citealt{MZ23}), where the DCs are the orange triangles and the stores are the azure blobs whose size is proportional to the corresponding request.
%\begin{figure}[ht]
%	\centering
%	\begin{minipage}{0.48\textwidth}
%		\centering
%		\includegraphics[width=\linewidth]{figs/gmap.png}
%		\vspace{0.5em}
%		\textit{(a) gmap view}
%	\end{minipage}
%	\hfill
%	\begin{minipage}{0.45\textwidth}
%		\centering
%		\includegraphics[width=\linewidth]{figs/qgis.png}
%		\vspace{0.5em}
%		\textit{(b) GIS view}
%	\end{minipage}
%	\caption{Stores and distribution centers locations.}
%	\label{fig:locations}
%\end{figure}
Each store must be allocated to a distribution center (DC), ensuring that the total requests made to a DC do not exceed its storage capacity. It is assumed that the handling and transportation capacities will always be sufficient. Both storage capacities and requests are quantified in terms of pallets, with no further granularity at this analytical level. The cost of servicing a store from a DC is assessed based on the travel time required to transport goods between the two entities. Notably, the three smaller DCs are fully owned by the company, whereas the fourth DC is situated in a leased facility.

The ultimate goal of the analysis is to determine how much space to rent during the next peak demand period, the Christmas season. To achieve this, we first optimize the allocation of stores to DCs, taking into account transportation and storage costs, and from the solution it is immediate to derive the space required in each DC, therefore the space to rent in the largest one.

Another requirement characterizes the problem we are addressing: large stores located near small DCs often lead to rapid saturation of the capacity of central DCs. This forces nearby stores to be served by comparatively distant DCs. Therefore, it has been proposed to allow tentative split service for certain selected stores to facilitate what-if analysis. The problem we address thus involves a parametric number of possible assignments for each store, where which store should be allowed multiple (double) assignments is an input parameter, not a decision variable.  

The analysis was conducted during the summer of 2022 in preparation for the subsequent Christmas season. At the time of the analysis, the anticipated demand requests were unknown and required forecasting. The analysis underwent iterative refinements throughout the summer, culminating in the final forecast produced in September. Consequently, the data presented in this paper correspond to a three-month forecasting case. 

The data for this test case were proposed by \citet{MZ23}, but that previous work took a fundamentally different approach, consisting of a Lagrangian matheuristic applied to a deterministic model of a different problem, with different objective and constraints to those considered here. This paper therefore presents a genuinely novel approach that greatly expands the significance of the test case. The extended benchmark set is also entirely new.

All codes and data are available in an anonymized format from the project repository 
%\citep{GDOforecastRepo}
{\it (public upon acceptance)}. Specifically, the file containing the request time series can be downloaded from the repository, where all series are complete up to the most recent actual values. However, it should be noted that the last three values were unknown at the time the final forecast was generated. 

According to the classification proposed in \citep{DEKOK2018} our contribution is described by the following typology string: 2,D,D,G|F,C|D,G|b,F,N|SC|E,O
standing for: {\it System specification}: 2 echelons, Divergent material structure, Discrete events, Global information; {\it Resources specifications}: bounded storage, Constant delivery time; {\it Market specifications}: Discrete stochastic demand, Guaranteed service; {\it Control type specifications}: installation base stock policy, Flexible release quantities, No other means to satisfy unexpected requirements; {\it Performance specifications}: meeting service requirements and minimization of costs; {\it Generic scientific aspects}: Exact techniques, Optimization.

% \subsection{Data preprocessing}  \label{Sec:preproc}

% The provided time series data exhibited intermittent disruptions attributable to the impact of the COVID-19 pandemic.  A marked decline in sales was observed during the three-month period of complete lockdown commencing in the second year of the observational period.  Given the relatively short length of the time series (45 months), direct inclusion of this period would introduce significant bias to subsequent forecasts, necessitating a data imputation strategy. To avoid the construction of a complex model specifically addressing this anomaly (which might introduce unwarranted assumptions),  we opted for a data exclusion approach followed by reconstruction using established time series methods.

% Various imputation techniques exist, ranging from basic methods such as forward or backward filling \citep{ref1} to more sophisticated approaches, including spline interpolation \citep{ref2}.  Given the observed pronounced yearly seasonality coupled with an apparent linear trend in the data, the latter approach was selected.  This assumption of linearity was validated statistically through an ANOVA F-test comparing a linear model to a quadratic model for each series. A p-value consistently exceeding 0.05 across all series confirmed the suitability of the linear trend assumption.

% Subsequently, seasonal decomposition \citep{CCMT90} was employed to address data incompleteness. The time series for each store was detrended and the seasonal coefficients were determined via averaging of the deviations from the trend, using only the available data.  This enabled the reconstruction of the missing data points during the lockdown period, yielding a preprocessed dataset suitable for reliable forecasting model application (Figure 2). This method is preferred for its simplicity and avoidance of model complexities associated with handling outliers or non-stationarity.

% \begin{figure*}[htp] 
	%     \centering
	%     \subfloat[Empirical data.]{%
		%         \includegraphics[width=0.5\textwidth]{serie_covid.png}%
		%         \label{fig:covida}%
		%         }%
	%     \hfill%
	%     \subfloat[Preprocessed data]{%
		%         \includegraphics[width=0.5\textwidth]{serie_nocovid.png}%
		%         \label{fig:covidb}%
		%         }%
	%     \caption{Input data.} % Main caption
	%     \label{fig:test4}
	% \end{figure*}

\section{The predictive module} \label{Sec:forecasting}

Data-driven approaches to estimating the distribution of stochastic variables have gained increasing attention as alternatives to classical parametric modeling, particularly in settings where assumptions of stationarity or known distributional forms are invalid. Unlike traditional techniques that rely on a priori specification of distribution families (e.g., Gaussian or Poisson), data-driven methods infer distributional characteristics directly from historical or observed data, making them especially suited for real-world applications with complex, noisy, or limited information.

Among the most prominent techniques are resampling-based methods such as the Bootstrap \citep{b1}, which generates empirical distributions by repeatedly sampling from the data with replacement. The Maximum Entropy Bootstrap (MEB) \citep{Vinod2006} extends this idea by producing replicates that preserve dependence structures in time series data, making it particularly valuable for short or non-stationary sequences.

Recent work has explored the integration of these empirical distribution estimates into stochastic and robust optimization pipelines. Notable contributions include nonparametric scenario generation \citep{BM11}, distributionally robust optimization using empirical Wasserstein distances \citep{EK17}. 

In our proposal, we utilize the MEB methodology in conjunction with the ensemble learning technique known as {\it bagging} (Bootstrap Aggregating). This combination enhances the accuracy and robustness of predictive models through the generation of multiple bootstrapped datasets from the original data. By training individual models on these datasets and aggregating their predictions through averaging, we adhere to MEB's principles while capturing the underlying statistical variability. Maximizing entropy under empirical constraints allows the integration of bagging with MEB to produce more robust models. In more details, each model trained on distinct bootstrapped samples incorporates uncertainty characteristics from the original data, allowing for a comprehensive representation of its variation.

The incorporation of MEB into the bagging framework not only bolsters the performance of predictive models but also facilitates uncertainty quantification in the predictions. Each model produces a set of predictions that can be used to construct confidence intervals, thereby offering insights into the variability and reliability of the forecasted outcomes. The ability to assess the spread of predictions from multiple bootstrapped datasets presents a better understanding of potential prediction intervals, which is particularly valuable in the context of the present paper, characterized by short, non-stationary, and possibly non linear time series surrounded by a significant amount of uncertainty.

Furthermore, this approach allows for dealing with noisy datasets and mitigating overfitting, as bagging inherently provides a mechanism to increase the robustness of the predictive model by leveraging the diversity across the bootstrapped samples. Since MEB generates bootstrapped datasets based on the maximum entropy principle, the resulting models avoid being overly sensitive to any one particular realization of the data.

Mathematically, if we denote the bootstrapped replicas of the dataset as $D^*_1, D^*_2, \ldots, D^*_B$, where $B$  represents the number of bootstrapped samples, the resulting predictive framework can be expressed as:  $\hat{y}_{\textit{BAG}} = \rho( \hat{y}_{b}(x) )$, where $\hat{y}_{b}(x)$ is the prediction made by the $b-th$ model trained on the bootstrapped sample $ D^*_b $  and $\rho$ a generic centrality parameter. 
This averaging process effectively reduces variance by smoothing out the predictions of the individual models, leading to enhanced stability and generalization capabilities in the final aggregated model.  The parameter $\rho$ can be defined as either a simple average or the median, which leads to the following bagging predictors:

\begin{equation}
	\hat{y}_{\textit{BAG}}  = \frac{1}{B} \sum_{b=1}^{B} \hat{y}_{b}(x)
\end{equation}

\begin{equation}
	\hat{y}_{\textit{BAG}}  = 
	\begin{cases} 
		\hat{y}_{b}(x) \left[\frac{h+1}{2}\right] & \text{if } h \text{ is odd} \\ 
		\frac{\hat{y}_{b}(x)\left[\frac{h}{2}\right] + \hat{y}_{b}(x)\left[\frac{h}{2} + 1\right]}{2} & \text{if } h \text{ is even} 
	\end{cases}
\end{equation}

\noindent where, $\hat{y}_{b}(x)$  is  sorted in ascending order whereas $h$ is the number of future values to be predicted.

\subsection{Resampling techniques} \label{Subsec:resampling}

%In recent years, resampling techniques, particularly the bootstrap method, have gained prominence as valuable tools for mitigating the inherent limitations of conventional forecasting approaches \citep{hasni2019spare}.  The bootstrap method, a non-parametric procedure, generates multiple simulated time series through resampling with replacement from the observed data. This methodology proves particularly advantageous when confronting the complexities of non-stationary, non-linear, intermittent and asymmetric cyclical patterns as well as scenarios characterized by a low signal-to-noise ratio \citep{chandran2024strategic,feliu2024enhancing,suman2024deep,zhang2024forecasting,kreiss2012bootstrap}.  Furthermore, resampling methods offer considerable utility in mitigating the effects of limited sample sizes \citep{dominguez2022machine} and in addressing model uncertainty \citep{sarris2020exploiting}. This provides logistics organizations with a more nuanced understanding of forecast uncertainty, enabling more precise risk management strategies and facilitating the adoption of more confident, data-driven decision-making processes \citep{ferrari2018bootstrap}.  The utility of bootstrap methods in statistical model selection has been extensively demonstrated through various studies. Fenga and Politis \citep{fenga2011bootstrap} introduced a bootstrap-based approach for autoregressive moving average (ARMA) order selection, showcasing its effectiveness in enhancing model accuracy and reliability in the presence of finite sample sizes. Their subsequent work \citep{fenga2013bootstrap} extended this methodology to threshold autoregressive models (SETAR), further validating bootstrap techniques as a robust mechanism for determining optimal model specifications. Moreover, Fenga and Politis \citep{fenga2017lasso} explored the application of the LASSO technique for sparse autoregression, underscoring a bootstrap framework that effectively addresses model selection in high-dimensional contexts. Collectively, these studies highlight how bootstrap methods can significantly improve order selection processes across various statistical modeling frameworks, demonstrating their broader applicability and relevance in contemporary statistical analysis.

In recent years, resampling techniques, especially the bootstrap method, have become valuable for overcoming the limitations of conventional forecasting methods \citep{hasni2019spare}. The bootstrap method creates simulated time series through resampling from observed data. This approach is useful for non-stationary, non-linear, intermittent, and asymmetric cyclical patterns, as well as low signal-to-noise scenarios \citep{feliu2024enhancing, kreiss2012bootstrap}. Moreover, resampling methods help address the effects of limited sample sizes \citep{davison1997bootstrap}  and model uncertainty \citep{sarris2020exploiting}. 

The utility of bootstrap methods in model selection has been extensively validated. Fenga and Politis \citep{fenga2011bootstrap} introduced a bootstrap-based approach for autoregressive moving average (ARMA) order selection, demonstrating its effectiveness in enhancing model accuracy with finite sample sizes. Furthermore, they explored the LASSO technique for sparse autoregression, illustrating a bootstrap framework for model selection in high-dimensional contexts \citep{fenga2017lasso}. These studies collectively show how bootstrap methods can enhance order selection across statistical modeling frameworks, underscoring their relevance in modern statistical analysis.

%The resampling scheme adopted here is called Maximum Entropy Bootstrap (MEB) \citep{meb1}. As it will be explained, MEB offers a principled and flexible approach to bootstrapping time series data, addressing many of the limitations of traditional block bootstrap methods. Its ability to handle complex dependencies and non-stationarity, coupled with its robust performance, makes it a valuable tool for a wide range of time series analysis tasks This powerful nonparametric resampling technique has revolutionized time series analysis by offering a principled and elegant approach to constructing bootstrap distributions \citep{meb2,meb3}. Unlike traditional block bootstrap methods (see, e.g., \citep{bb1,bb2}), which resample blocks of data to preserve the serial correlation structure, MEB seeks to maximize the entropy of the bootstrap distribution subject to constraints derived from the observed data. This approach leads to more flexible and less restrictive bootstrap distributions, offering several advantages over traditional methods, especially when dealing with complex time series data. A critical aspect of MEB’s application is its robustness in nonstationary environments, where traditional forecasting methods often falter. MEB can capture the evolving dynamics of the time series, thereby enhancing the predictive accuracy of the forecasts \citep{chen2020}. By generating multiple bootstrap samples, MEB facilitates the construction of confidence intervals and prediction intervals, which are paramount for effective decision-making in fields like finance and risk management \citep{shafer2019}. Furthermore, MEB allows for the incorporation of complex models, such as those involving seasonal patterns or autoregressive structures, into the resampling framework. This flexibility enables researchers to model intricate temporal dynamics, making MEB particularly suitable for various applications ranging from economic forecasting to climate modeling. Its capacity to integrate both linear and nonlinear dependencies provides a significant advantage over traditional methods that may oversimplify these relationships \citep{lopez2023}.

The standard bootstrap assumes independent and identically distributed (i.i.d.) data — an assumption that fails for time series, where observations are dependent and ordered.
This failure is more pronounced when the series is short, making dependence patterns more influential, the data exhibits autocorrelation, seasonality, or trend and when resampling destroys the structure and underrepresents variability.
In these cases Maximum Entropy Bootstrap (MEB) guarantees the following advantages:

\begin{itemize}
	\item {\it Preservetion of Dependence Structure } MEB maintains the temporal dependence (e.g., autocorrelation, trend, seasonality) by using rank-based and interpolation techniques that reorder and smooth the data. Unlike i.i.d. bootstrap, it doesn’t break time series structure.
	
	\item {\it Better Handling of Short Time Series } In short samples, structural properties (e.g., local trends) are easily lost by standard resampling. MEB keeps these features by reconstructing plausible time series paths that are consistent with observed data and uncertainty.
	
	\item {\it No Need for Model Specification} Unlike block bootstrap or parametric resampling, MEB is nonparametric and doesn’t require selecting block sizes or fitting ARMA models — a key benefit for short samples where such models may be unstable or overfitted.
	
	\item {\it Generation of Smooth Pseudo-Series} MEB produces continuous, smoothed pseudo-series through linear interpolation, making it well-suited for statistical inference tasks like forecasting, confidence intervals, and hypothesis testing.
	
	\item {\it Maximum Entropy Principle} The generated bootstraps are the least biased (i.e., most non-committal) estimates consistent with the known constraints (the observed data). This is especially valuable when little information is available — like in short series.
	
	\item {\it Support of Stationary and Non-Stationary Series} MEB is flexible with both stationary and non-stationary time series, whereas other bootstrap techniques often require stationarity or explicit transformation to achieve it.
\end{itemize}

The MEB resampling scheme adopted here offers a principled and flexible approach to bootstrapping time series data, leveraging the {\it Maximum Entropy Principle} (MEP) \citep{shannon1948mathematical} to address the inherent limitations of conventional bootstrap methods, thus addressing many limitations of traditional block bootstrap methods. The essence of MEP is grounded in the idea that in the absence of specific information, the most rational probability distribution to adopt is one that maximizes entropy, reflecting the greatest uncertainty about the system \citep{jaynes1957information}. This is mathematically articulated through Shannon entropy,  defined as $S(p) = -\sum_{i=1}^{n} p_i \log(p_i)$, where $ p_i$ denotes the probability associated with the i-th outcome within a discrete sample space of size $n$. 
% Its ability to handle complex dependencies and non-stationarity, along with robust performance, makes it valuable for a wide range of time series analysis tasks. This powerful nonparametric resampling technique has transformed time series analysis by providing an elegant method for constructing bootstrap distributions \citep{meb2, meb3}. 

% Unlike traditional block bootstrap methods \citep{bb1, bb2}, which resample data blocks to preserve serial correlation, MEB maximizes the entropy of the bootstrap distribution subject to constraints from the observed data. This results in more flexible bootstrap distributions, offering advantages over traditional methods, especially for complex time series data. A critical aspect of MEB is its robustness in nonstationary environments, where traditional forecasting methods may struggle. MEB captures evolving time series dynamics, enhancing forecast accuracy \citep{chen2020}. By generating multiple bootstrap samples, MEB facilitates the construction of confidence and prediction intervals, crucial for effective decision-making in finance and risk management \citep{shafer2019}. Furthermore, MEB accommodates complex models involving seasonal patterns or autoregressive structures, enabling researchers to model intricate temporal dynamics. Its ability to integrate linear and nonlinear dependencies provides a significant advantage over traditional methods that may oversimplify relationships \citep{lopez2023}.

% The Maximum Entropy Bootstrap (MEB) \citep{Vinod1993,Vinod2006,Kourentzes2015,Vinod2008} represents a systematic approach to resampling that integrates the principles of information theory with statistical inference, leveraging the Maximum Entropy Principle (MEP) \citep{shannon1948mathematical} to address the inherent limitations of conventional bootstrap methods. The applications of MEB extend across various disciplines such as econometrics, ecology, and social sciences, wherein empirical data often suffer from limitations such as small sample sizes or biased distributions. The flexibility of MEB allows researchers to incorporate prior knowledge through constraints while maximizing uncertainty and ensuring that the bootstrap samples reflect a realistic representation of the original dataset. MEB offers a compelling alternative to traditional bootstrap methodologies—especially in scenarios where the underlying distribution is unknown or when the sample data is not sufficiently large to assume a normal distribution. The essence of MEP is grounded in the idea that in the absence of specific information, the most rational probability distribution to adopt is one that maximizes entropy, reflecting the greatest uncertainty about the system \citep{jaynes1957information}. This is mathematically articulated through Shannon entropy,  defined as $S(p) = -\sum_{i=1}^{n} p_i \log(p_i)$, where $ p_i$ denotes the probability associated with the i-th outcome within a discrete sample space of size $n$. 

The pivotal advancement provided by MEB lies in its ability to formulate bootstrapped samples that adhere to a spectrum of constraints dictated by the original dataset, such as predetermined moments, probabilities, or other statistical characteristics \citep{berger1985statistical}. Specifically, under typical circumstances, these constraints can be specified in the form of normalized probabilities and empirical expectations, expressed as $\sum_{i=1}^{n} p_i = 1$ to ensure that the probabilities form a valid distribution, and $\sum_{i=1}^{n} p_i f_i = \bar{f}$, where $f_i$ represents measurements or statistics relevant to the analysis, with $\bar{f}$ being the empirical mean of these measurements. The formulation of the Maximum Entropy problem consequently leads to a constrained optimization framework. The optimization problem can be succinctly encapsulated in the Lagrangian:
\begin{equation}
L(p, \lambda, \mu) = S(p) + \lambda \left(1 - \sum_{i=1}^{n} p_i\right) + \mu \left(\bar{f} - \sum_{i=1}^{n} p_i f_i\right),
\end{equation}
where $\lambda$ and $\mu$ are Lagrange multipliers that adjust the balance under the constraints of normalizing the distribution and matching the empirical mean, respectively. The solution to this maximization problem yields a probability distribution, which appears as
\begin{equation}
p_i = \frac{e^{\mu f_i}}{\sum_{j=1}^{n} e^{\mu f_j}},
\end{equation}
where the parameter $\mu$ is determined through the constraint equations, specifically via a numerical root-finding approach that satisfies the moment condition $\sum_{i=1}^{n} p_i f_i = \bar{f}$.

Building on this classical formulation, we define an Objective-Augmented Maximum Entropy Bootstrap (OA-MEB) scheme that incorporates additional optimization-oriented structure directly into the resampling distribution. To do so, we introduce an auxiliary objective-related component $g_i$, representing scenario-wise quantities that are relevant to downstream decision or cost functions in stochastic programming (e.g., expected cost contributions, constraint violations, or risk-sensitive statistics). The OA-MEB extends the entropy maximization problem by embedding a tunable scalar parameter $\gamma \ge 0$ that controls the influence of this additional term while preserving the probabilistic and moment-matching constraints. The resulting augmented Lagrangian is given by:
\begin{equation}
L_{\mathrm{OA}}(p, \lambda, \mu, \gamma) = S(p) 
+ \lambda \left( 1 - \sum_{i=1}^{n} p_i \right)
+ \mu \left( \bar{f} - \sum_{i=1}^{n} p_i f_i \right)
+ \gamma \left( \sum_{i=1}^{n} p_i g_i \right),
\end{equation}
where the additional inner product term $\sum_{i=1}^{n} p_i g_i$ promotes bootstrap resamples that remain statistically coherent while incorporating optimization-driven scenario emphasis. Solving the first-order optimality conditions yields a modified exponential-tilting solution:
\begin{equation}
p_i^{\mathrm{OA}} = \frac{\exp\left(\mu f_i + \gamma g_i\right)}
{\sum_{j=1}^{n} \exp\left(\mu f_j + \gamma g_j\right)},
\end{equation}
showing that classical MEB is recovered as the special case $\gamma = 0$. Calibration of $\gamma$ governs the trade-off between purely information-theoretic sampling and decision-aligned resampling, and may be performed via cross-validation, bilevel optimization, or risk-preference calibration strategies consistent with the downstream stochastic programming model.

This formulation transforms the problem into one that allows for flexible adaptation to various types of data and constraints imposed by the empirical observations. Once this extended maximum entropy distribution is established, resampling via bootstrapping can be accomplished by selecting  $m$ samples according to the derived $ p_i$.  This produces synthetic datasets that not only reflect the variability and uncertainty characteristic of the original data but also adhere to the specific constraints imposed, which may include higher moments or even domain-specific statistics. 

This resampling process can be iterated to generate a comprehensive set of bootstrapped samples, enabling a multitude of statistical analyses, including confidence interval estimation, hypothesis testing, and model validation \citep{efron1993introduction}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The prescriptive module} \label{Sec:optimization}

The predictive module provides a structural component of the prescriptive module, which aims to optimize the allocation of stores to distribution centers (DCs), thereby enabling the quantification of space to rent in third-party DCs. The results of the optimization are in turn used to evaluate the modeled {\it scenario}, where scenarios are defined for specific what-if analyses. For example, the objective of the what-if analyses could be to examine the effect of freezing some allocations while leaving free the others, or possibly accepting some multiple -- double in the test case -- allocations for some selected stores. 

The core optimization problem is a {\em split allocation problem}. This variant of the basic allocation problem has already received attention in the optimization literature, mainly in the context of survivable communication network design \citep{EK98,MKE75}, and more generally as the splittable capacitated multiple allocation hub location problems \citep{M05}. It finds applications in fiber optic access networks \citep{KLH11}, split delivery routing \citep{DT89}, and even in school timetabling \citep{BBL07}. The problem we are interested in is related to one that is presented in \citep{ZHHY21}, but it is modelled in a very different way.

The mathematical model we use is a direct extension of the standard model for allocation problems. As our proposed framework is valid beyond the specific case study described in the paper, we will refer to the application elements (stores and data centers) using standard allocation terminology in what follows. To emphasize the generality of the analysis, we will use the more generic term 'clients' to denote the stores and 'servers' to denote the centers. In our context, assignment constraints can accept a parametric number of assignments, enabling requests to be serviced across multiple servers. The notation used in the model is as follows:

\begin{enumerate}[noitemsep, topsep=3pt]
	\item[$n$] number of clients (stores)
	\item[J] index set of clients, $J=\{1, \ldots, n\}$
	\item[$m$] number of servers (DCs)
	\item[I] index set of servers, $I=\{1, \ldots, m\}$
	\item[$c_{ij}$] global cost for allocating client $j \in J$ to server $i \in I$
	\item[$Q_i$] storage capacity of server $i \in I$
	\item[$d_i$] unit storage cost of server $i \in I$
	\item[$req_j$] total request of client $j$.
	\item[$b_j$] maximum number of servers (DCs) client $j$ can be assigned to
	\item[$x_{ij}$] binary decision variable equal to 1 iff client $j$ is allocated to server $i$, $i \in I$ and $j \in J$
	\item[$q_{ij}$] integer decision variable representing the amount (number of pallets) of request of client $j$ provided from server $i$
\end{enumerate}

In the final test case, the amount requested by each client was always set to equal the corresponding predicted value. However, in some scenarios, some clients' requests could be split between two servers, meaning the requested $q_(ij)$ amounts could vary.

The mathematical model used for this problem was as in the following formulation FD.

\begin{align}
	&(FD) & z_{FD} = \textbf{min } & \sum_{i \in I} \sum_{j \in J} ( c_{ij}x_{ij} + d_i q_{ij} ) \label{ALLobj}\\
	& &  \textbf{subject to }
	& \sum_{i \in I} q_{ij} = req_j & j \in J  \label{ALLrequest}\\
	& &  & \sum_{j \in J}q_{ij} \leq Q_i & i \in I \label{ALLcapacity}\\
	& &  & \sum_{i \in I} x_{ij} = b_j & j \in J  \label{ALLnumserv}\\
	& &  & q_{ij} \leq req_j x_{ij} & i \in I, j \in J \label{ALLcontin}\\
	& &  & x_{ij} \in \{0,1\} & i\in I, j \in J \label{ALLx}\\
	& &  & q_{ij} \in \mathbb{Z}^+_0 & i\in I, j \in J \label{ALLq}
\end{align}

The constraints in equation \ref{ALLnumserv} are formulated as equalities due to the need to impose specific splits in 'what if' scenarios. However, if this is not of particular interest, these constraints could be replaced by less-than-or-equal-to inequalities combined with inequalities that ensure allocations are made to at least one server.

Note that when $b_j = 1$ for all clients, i.e., when each client $j \in J$ can be served by only one server, constraints \ref{ALLcontin} and \ref{ALLrequest} force the entire quantity requested by client $j$, $req_j$, to be supplied by a single server. The constraints \ref{ALLcontin} thus become equations, the constraints \ref{ALLrequest} become redundant, and the constraints \ref{ALLcapacity} can be expressed by replacing $\sum_{j \in J}q_{ij}$ with $\sum_{j \in J}req_j x_{ij}$. Problem P reduces to the standard GAP and can be solved by any of the methods described in \citet{matheuristics}.

However, we are interested in the general case that allows for multiple assignments. Since the cost is derived from these assignments, the problem can be considered a fixed-cost split-assignment allocation problem.
Furthermore, as future client requests can only be forecast, it is more convenient to formulate the problem as a stochastic optimization problem, where the future requests are random variables $\rho_j$, $j \in J$, whose distributions are derived from the forecast results. The resulting stochastic problem is formulated as follows.

\begin{align}
	&(FS) & z_{FS} = \textbf{min } & \sum_{i \in I} \sum_{j \in J} ( c_{ij}x_{ij} + d_i q_{ij} ) \label{SALLobj}\\
	& &  \textbf{subject to }
	& \sum_{i \in I} q_{ij} = R(\rho_j) & j \in J  \label{SALLrequest}\\
	& &  & \sum_{j \in J}q_{ij} \leq Q_i & i \in I \label{SALLcapacity}\\
	& &  & \sum_{i \in I} x_{ij} = b_j & j \in J  \label{SALLnumserv}\\
	& &  & q_{ij} \leq R(\rho_j) x_{ij} & i \in I, j \in J \label{SALLcontin}\\
	& &  & x_{ij} \in \{0,1\} & i\in I, j \in J \label{SALLx}\\
	& &  & q_{ij} \in \mathbb{Z}^+_0 & i\in I, j \in J \label{SALLq}
\end{align}

\noindent where the values $R(\rho_j)$ in the constraints \ref{SALLrequest} and \ref{SALLcontin} refer to any same realization of the random variable $\rho_j$.

Different methods can be used to deal with this case \citep{SDR09}. We transformed the formulation (FS) into its {\it deterministic equivalent} \citep{D55} using the forecasts computed on the bootstrap sets. We generated multiple scenarios each containing a realization of the stochastic variable $\rho$, which is defined as a forecast of one series from the bootstrap set in use. For each scenario $s$, we thus created a version of the constraint with $b_i^{s}$ as the right-hand side. The final solution must satisfy these constraints across all scenarios.

The objective function now incorporates the quantities to be delivered to each client in different scenarios. Therefore, we consider the expected cost as a weighted average across these scenarios and include it in the objective function. 
The resulting formulation is in formulation (DE). The random variables $\rho_j$ are sampled in a finite number of scenarios. Let $\rho^s_j$ denote the realization of $\rho_j$ in scenario $s$, $s \in S$, and let $p_s = \frac{1}{|S|}$ be the probability associated with scenario $s \in S$. The variables $\epsilon_{js}$ are slack variables for the customer service constraints.

\begin{align}
	&(DE)\quad z_{DE} = & \textbf{min} &\sum_{s \in S} \left( p_s \sum_{j \in J} \left( \sum_{i \in I} ( c_{ij}x_{ij} + d_i q_{ij}^s) + M \epsilon_{js} \right) \right) \label{DEobj}\\
	& &  \textbf{s.t.}
	& \sum_{i \in I} q_{ij}^s + \epsilon_{js} = \rho_j^s & j \in J, s \in S \label{DErequest}\\
	& &  & \sum_{j \in J}q_{ij}^s \leq Q_i & i \in I, s \in S \label{DEcapacity}\\
	& &  & \sum_{i \in I} x_{ij} = b_j & j \in J  \label{DEnumserv}\\
	& &  & q_{ij}^s \leq \rho_j^s x_{ij} & i \in I, j \in J, s \in S \label{DEcontin}\\
	& &  & x_{ij} \in \{0,1\} & i\in I, j \in J \label{DEx}\\
	& &  & q_{ij}^s \in \mathbb{Z}^+_0 & i\in I, j \in J, s \in S \label{DEq}\\
	& &  & \epsilon_{js} \in \mathbb{Z}^+_0 & j \in J, s \in S \label{DEeps}
\end{align}

In this formulation, since each variable $\rho_j^s$ derives from a specific realization of the corresponding random variable $\rho_j$, the amounts to be delivered to each store $j \in J$ are scenario-specific. However, it is required to determine a unique allocation that allows to serve all stores in any proposed scenario. This last requirement may be infeasible, so the slack variables $\epsilon_{js}$ are needed in the formulation. They are lexicographically penalized in the objective function by appropriately high costs, in order to implement the primary concern of achieving feasibility.

Optimizing formulation (DE) makes it possible to determine the quantities that each server must be able to supply to each client, i.e. the total quantity that each server must handle. 
In terms of our logistical application, since each DC must maintain sufficient inventory to meet the anticipated demand during the designated period, the optimization of the client (retailer) allocation translates directly into a quantification of the DC inventory and therefore of its required minimum size. Thus, solving the allocation problem also solves the dimensioning question that motivated this research.

\section{Computational results} \label{Sec:results}

This section reports on the results of validating the predictive and prescriptive modules in accordance with the specifications previously described.

\subsection{Predictive module tests}    \label{Subsec:predictive}

The objective of the predictive module is to forecast demand for each store, asking for a forecast three months ahead.
The available data consists of a time series for each of the 52 stores. Each series contains 45 values corresponding to four years of monthly data on relevant requests for each store. The values for the last three months of the last year are missing, as these are to be forecast.

The core approach set out in this paper is the bootstrap method, which is described in Section \ref{Sec:forecasting}. The results obtained using this method were validated against those obtained using several alternative forecasting algorithms: SARIMAX, Holt-Winters, MLP, RNN (LSTM), Random Forest and Gradient Boosting (XGBoost). The relevant hyperparameters were determined by grid search for algorithms with integer-valued parameters, such as SARIMAX, and by Hammersley sampling \citep{H60} for real-valued parameters.

All series were pre-processed using a 'log-diff' transform to approach stationarity, and a max–min scaler was applied to the artificial intelligence-derived methods. Python version 3.11 was used for all implementations and the codes were run on a Windows PC workstation equipped with four Intel Core i7-4790 CPUs running at 3.60 GHz and with 32 GB of RAM.

The focus of this part of the research is on the bootstrap method. To this end, we tested the approach with the following settings:
\begin{itemize}
	\item bootstrap sets containing 75, 125 and 175 series;
	\item bootstrap sets generated by simple autoregressive AR(p) with p=5 model (the default in python's statsmodels), where parameters are estimated using Yule-Walker equations \citep{BD91}, and by ARIMA, where autoarima is run on each of the seed series and the optimized orders are then applied to each boosted series;
	\item the forecasting of all the boosted series generated as above to obtain the boosted forecasts was tested with AR($p$), Random Forest or ARIMA. In the case of ARIMA, autoarima was only repeated on the first series of each bootstrap set. In the case of AR, the parameter $p$ was set for each series after a loop on different $p$ values, keeping the one that maximized the corresponding AIC value.
\end{itemize}

We also attempted to generate bootstrap sets with and without backcasting, as well as with simple extraction and repetitions among the residuals of the seed series, and compared these with their scrambling. We did not report the results of these tests because backcasting worsened the results, while scrambling the residuals showed no difference compared to extraction with repetitions. Therefore, in line with the literature, we decided to use the latter approach.

The forecasts were compared using three cost functions: mean absolute error (MAE), mean squared error (MSE) and bias. The results are reported by ranking the corresponding forecasts for each series according to the quality of the considered function to emphasize how the algorithms perform relative to each other. Then, the rank of each function is averaged over all series. Appendix \ref{app:errcost} reports the absolute values of the error measures, rather than the rank-based values, for all boostset sizes. 

Table \ref{table:YWARranks} presents the results of comparing the forecasts obtained using standard algorithms with those obtained using boosting. The boosted series were computed using an AR(5) model based on Yule-Walker equations. The forecasts of these series were obtained using a simple autoregressive model. The boosted sets consisted of 75, 125, and 175 series, which are labelled $YW\_AR\_75$ (standing for Yule-Walker, AutoRegressive, 75 series bootstrap set), $YW\_AR\_125$, and $YW\_AR\_175$, respectively.

The columns show the average ranking of each cost function (MAE, MSE or bias) for each boostset size, while on the rows we report:

\begin{itemize}
	\item $fcast\_avg$: boost forecast obtained as the average of the corresponding ranks;
	\item $fcast\_50$: boost forecast obtained as the median of the corresponding ranks;
	\item $yar$: non-boost forecast obtained by an AR model;
	\item $yarima$: non-boost forecast obtained by an ARIMA model;
	\item $yhw$: non-boost forecast obtained by a Holt and Winters model;
	\item $ymlp$: non-boost forecast obtained by a multilayer perceptron model;
	\item $ysvm$: non-boost forecast obtained by a SVM model;
	\item $ylstm$: non-boost forecast obtained by a LSTM model;
	\item $yrf$: non-boost forecast obtained by a random forest model;
	\item $yxgb$: non-boost forecast obtained by a XGboost model;
\end{itemize}

\vspace{0.3cm}
\begin{table}
	\centering
	\renewcommand{\arraystretch}{0.9}
	\begin{tabularx}{\textwidth}{l|CCC|CCC|CCC}
		& \multicolumn{3}{|c|}{YW\_AR\_75} &  \multicolumn{3}{|c|}{YW\_AR\_125}  &  \multicolumn{3}{|c}{YW\_AR\_175} \\
		& MAE & MSE & bias & MAE & MSE & bias & MAE & MSE & bias \\
		\hline
		fcast\_avg& 2.67 & 2.67 & 0.08 & 2.58 & 2.58 & 0.05 & 3.83 & 3.83 & 0.04 \\
		fcast\_50 & 3.04 & 3.04 & 0.06 & 3.19 & 3.19 & 0.04 & 2.50 & 2.50 & 0.03 \\
		yhw       & 5.42 & 5.42 & 0.08 & 6.06 & 6.06 & 0.04 & 5.88 & 5.88 & 0.03 \\
		yarima    & 5.73 & 5.73 & 0.05 & 6.04 & 6.04 & 0.03 & 6.08 & 6.08 & 0.02 \\
		ysvm      & 5.88 & 5.88 & 0.08 & 5.50 & 5.50 & 0.05 & 5.50 & 5.50 & 0.03 \\
		ylstm     & 6.02 & 6.02 & 0.06 & 5.52 & 5.52 & 0.04 & 6.27 & 6.27 & 0.02 \\
		yar       & 6.06 & 6.06 & 0.04 & 6.54 & 6.54 & 0.02 & 6.79 & 6.79 & 0.02 \\
		ymlp      & 6.38 & 6.38 & 0.05 & 5.98 & 5.98 & 0.03 & 6.17 & 6.17 & 0.02 \\
		yrf       & 6.46 & 6.46 & 0.11 & 6.31 & 6.31 & 0.06 & 5.48 & 5.48 & 0.04 \\
		yxgb      & 7.33 & 7.33 & 0.12 & 7.29 & 7.29 & 0.07 & 6.50 & 6.50 & 0.05 \\
		\hline
	\end{tabularx}
	\caption{Average rankings for different boostset size.}
	\label{table:YWARranks}
\end{table}

The table shows that boosting is consistently the most reliable method for minimizing forecast errors, even when forecasts are obtained from a model as simple as AR($p$), while ARIMA produces the least bias.

Figure \ref{fig:distr} shows two examples of forecast distributions obtained for two different series. The green bars correspond to the distribution of the results obtained by the AR[p] model over the differently recolored alternatives of a series. The distribution is superimposed with lines representing the results of other forecasting algorithms applied to the original series.

\begin{figure}[ht]
	\centering
	\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[width=\linewidth]{figs/distr_75_0.png}
		\label{fig:dista}%
		\vspace{0.5em}
		\textit{(a) 75 bootset}
	\end{minipage}
	\hfill
	\begin{minipage}{0.49\textwidth}
		\centering
		\includegraphics[width=\linewidth]{figs/distr_75_9.png}
		\label{fig:distb}%
		\vspace{0.5em}
		\textit{(b) 125 boostset}
	\end{minipage}
	\caption{Distribution of bootstrap forecasts.}
	\label{fig:distr}
\end{figure}

The relative merits of the different forecasting approaches are better highlighted by critical difference diagrams based on the non-parametric Friedman test followed by a post-hoc Nemenyi test. These diagrams allow for a visual comparison of performance ranks and indicate whether the differences between classifiers are statistically significant \citep{D06}. In our case, the graphs were generated using the SciKit version \citep{SciKitCD} and plot the average rank across all series for each algorithm along the x-axis. Horizontal bars connect those that could not be considered statistically different.

Figures \ref{fig:ywar75} and \ref{fig:ywar175} show a comparison of the ranks on MAE for the 75 series boostset and on MSE for the 175 series boostset. In both cases, as with all those tested, the accuracy of the boost forecasts is significantly higher than that obtained with alternative models. The tables in \ref{app:errcost} confirm that the comparison yields consistent results across all boostset sizes and cost functions, except for bias. The bias rankings are in fact unrelated to those obtained with the other cost functions. This is because the bias is very small in all cases, less than 1\% of the average of the empirical values, which ensures that all the forecasting models are free of systematic errors on our data and makes the difference in the rankings insignificant.

\begin{figure*}[ht] 
	\centering
	\includegraphics[width=0.75\linewidth]{figs/sp_MAE_res_YW_AR_75.eps}%
	\caption{Average ranks for MAE on instance res\_YW\_AR\_75}
	\label{fig:ywar75}
\end{figure*}

\begin{figure*}[ht] 
	\centering
	\includegraphics[width=0.75\linewidth]{figs/sp_MSE_res_YW_AR_175.eps}%
	\caption{Average ranks for MSE on instance res\_YW\_AR\_175}
	\label{fig:ywar175}
\end{figure*}

Our relatively short time series require a forecasting approach that can handle both limited data and non-stationarity.  Although sophisticated machine learning models such as recurrent neural networks (RNNs) and gradient boosting machines (GBMs) have become more popular in recent years, they can perform less well when there is limited data available, particularly when the data has non-stationary properties. This can lead to overfitting.  The autoregressive (AR) model combined with maximum entropy bootstrap offers a powerful and promising alternative.  This technique performed excellently in the context of this study, consistently outperforming more complex machine learning models. It offers an approach based on robust statistical principles that mitigates the risk of overfitting inherent in many AI approaches. Moreover, the AR model combined with the bootstrap methodology provides reliable forecasts ideally suited to integration into the subsequent optimization module.  

In our case, the choice of an autoregressive (AR) model not only offers superior accuracy, but also several crucial advantages over more complex machine learning approaches. Firstly, the AR model's inherent simplicity makes it easy to implement and interpret.  Unlike many machine learning algorithms, which operate as 'black boxes', the parameters of the AR model have clear statistical interpretations that provide insight into the underlying dynamics of the time series data. Secondly, the well-established theoretical framework of the AR model and the readily available diagnostic tools allow for robust validation and assessment of model performance.  

\subsection{Prescriptive module tests}  \label{Subsec:prescriptive}

The bootstrap forecasts results reported in section \ref{Subsec:predictive} provided the basis for instantiating the optimization phase. The prescriptive module was first run in a deterministic setting, to define a benchmark solution, then extended to the stochastic setting described in section \ref{Sec:optimization}.

When the problem is modelled using the (FD) formulation and the average predictions obtained by boosting are used as deterministic data, it can be easily solved by any state-of-the-art MILP solver. This demonstrates the feasibility of dealing with a significant deterministic equivalent of the stochastic formulation (FS). The optimal deterministic value of $z_{FP} =$ 15553 is used as a benchmark in the following tests.

We moved on to the (DE) formulation generating a value for each of the stochastic variables $\rho_j$ in correspondence to each bootstrap forecast.
%$|S|=100$ scenarios by randomly sampling the stochastic variables $\rho_j$ over the associated probability distributions of the values resulting from the forecasts on all series that are part of a boostset. 
Given these input data, we ran experiments to assess the impact of the variation of the remaining sets of parameters of the formulation, i.e., cardinality of the boostsets, number of splittable clients, and assumptions on inventory costs. 

All reported tests were run on the same machine as in subsection \ref{Subsec:predictive}, a Windows PC workstation equipped with four Intel Core i7-4790 CPUs running at 3.60 GHz and 32 GB of RAM, but in this case the implementation language was C++ to allow finer control over the MILP solver used, which was CPLEX v. 22.11.

This section reports on the sensitivity of the solution to variations in the instance coefficients. \ref{app:artificial} provides an initial study of the fitness landscape for the split-assignment problem, using a set of artificial benchmark instances.

\subsubsection*{Increasing boostset size}

The size of the boostset helps to stabilize the results during forecasting, but it also has a deep impact on the optimization. In fact, the number of variables and constraints in the DE formulation do not depend on the size of the boostset, larger boostsets are more likely to contain unusual values or combinations thereof. This makes the constraints in equation (\ref{DErequest}) progressively harder to satisfy, which increases the cost and may render the instance infeasible.

To ascertain this, we conducted experiments using 75, 125 and 175 series boostsets, with a time limit of 600 seconds on the optimization process. Figure \ref{fig:bsize} shows a boxplot diagram depicting the distribution of the optimized cost over a number of tests equal to 1/5 of the number of series in the boostset, i.e., 15, 25, 35, respectively.

\begin{figure*}[ht] 
	\centering
	\includegraphics[width=0.7\linewidth]{figs/figb0.eps}%
	\caption{Allocation cost for increasing boostset size}
	\label{fig:bsize}
\end{figure*}

Figure \ref{fig:bsize} has a red horizontal line at the cost value for the deterministic case. Therefore, the increase in cost can be considered a quantification of the value of information about storage requests. We observe that the cost remains relatively stable for the 75 and 125 series sets, but increases significantly for the 175 boostset. This is due to the increased difficulty of the larger boostset instances, which meant that some could only be solved heuristically within the time limit. While all instances were feasible, seven out of 35 still had a gap between the lower and upper bounds after 600 seconds of CPU time. 

Finally, we note that, as expected, taking the optimal solution obtained with the coefficients imposed by the 75 series and evaluating it with the coefficients of the 125 series results in a more costly solution than that computed with the 125 series coefficients. More interestingly, however, this solution is not feasible with the 175 series coefficients.

\subsubsection*{Increasing number of splittable clients}

A defining feature of the problem we face is the possibility of accepting split service for some clients. This is unusual for allocation problems and results from the requirement to perform specific what-if analyses on the results. The number of clients allowed to split, and which ones they are, is given in input. In the actual case, the service to a client, if splittable, could be split between at most two servers, and the quantities delivered had to be integers, as specified in all formulations.

The clients to be split are chosen from the most requesting clients. We ran a series of tests based on 75 series boostsets, allowing the 0, 4, 8, and 12 most requesting clients to be split. Figure \ref{fig:bmult} shows the corresponding boxplots.

\begin{figure*}[ht] 
	\centering
	\includegraphics[width=0.7\linewidth]{figs/figbmult.eps}%
	\caption{Allocation cost for increasing number of splittable clients}
	\label{fig:bmult}
\end{figure*}

Allowing a service to be split for a client is a partial relaxation of the corresponding assignment constraint, so the expected optimal cost is lower. This can indeed be seen in figure \ref{fig:bmult}, where the more clients that are permitted to split, the lower all distribution moments become. However, the marginal decrease becomes smaller as the number of clients who can split increases, since splitting a small client may result in only a small contribution, if any.

\subsubsection*{Impact of different inventory cost assumptions}

A further set of tests was conducted to determine the impact of inventory costs $d_i$, $i \in I$ on the search process. Recall that in the original case study, one of the objectives was to determine how much space should be rented in the only DC not owned by the company, where storage space needs to be leased. The data of the case study proposed in \citet{MZ23} did not report storage costs. Here, we consider storage costs that are inversely proportional to the average distance of the DC from the stores. We consider three settings, one with no storage costs, one where only the most distant DC incurs costs, and one where all DCs need to lease space. Costs are proportional to the quantities stored and are therefore influenced by the allocation policy.

All the tests were performed using the 75-series boost set. The focus of the analysis is not on the increase in optimal costs, since an increase in the costs of the decision variables clearly leads to an increase in the total costs. Rather, the focus is on the variation in the complexity of the search, as reflected in the CPU time required to reach the final result, and on the optimality or feasibility of the solutions. To evaluate the effectiveness of increasingly lengthy searches, we present results after 600, 1200, and 3600 seconds of CPU time.
The results are summarized in the table \ref{table:qcost}. All results are based on 15 repetitions, and the columns show:

\begin{itemize}[noitemsep,nolistsep]
	\item $n$: number of clients (stores);
	\item $m$: number of servers (DCs);
	\item $n_{boost}$: number of series in the boostset;
	\item $n_q$: number of servers incurring leasing costs;
	\item $n_{inf}$: number of infeasible solutions at the end of the search;
	\item GAP lin: average gap between the linear bound and the best solution at the end of the search;
	\item GAP fin: average gap between the best lower bound and the best solution at the end of the search;
	\item t.CPU: average CPU time (in sec) of the search process.
\end{itemize}

\vspace{0.3cm}
\begin{table}
	\centering
	\renewcommand{\arraystretch}{0.9}
	\begin{tabularx}{0.8\textwidth}{CCCC|CCCC}
		$n$ & $m$ & $n_{boost}$ & $n_q$ & $n_{inf}$ & GAP lin & GAP fin & t.CPU \\
		\hline
		52 & 4 & 75        & 0 & 0    & 0.01    & 0.00    &  333.95 \\
		\hline
		52 & 4 & 75        & 1 & 1    & 0.18    & 0.17    &   600  \\
		52 & 4 & 75        & 1 & 0    & 0.15    & 0.14    &  1200 \\
		52 & 4 & 75        & 1 & 0    & 0.14    & 0.13    &  3600 \\
		\hline
		52 & 4 & 75        & 2 & 0    & 0.04    & 0.04    &   600 \\
		52 & 4 & 75        & 2 & 0    & 0.01    & 0.03    &   1200 \\
		52 & 4 & 75        & 2 & 0    & 0.01    & 0.03    &   3600 \\
		\hline
		52 & 4 & 75        & 3 & 0    & 0.01    & 0.00    &  211.03 \\
		\hline
		52 & 4 & 75        & 4 & 0    & 0.01    & 0.00    &  13.00 \\
		\hline
	\end{tabularx}
	\caption{Gap and CPU time for increasing number of leased servers.}
	\label{table:qcost}
\end{table}

Table \ref{table:qcost} illustrates an interesting feature of the fitness landscape of this split-cost allocation problem: the complexity of the search is not monotonic with respect to the number of leased servers. In fact, CPLEX was able to solve all instances to optimality for 0, 3 and 4 leased servers. However, for 1 or 2 leased servers, it was never able to prove the optimality of the best solution found within the time limit of 3600 seconds. Furthermore, instances with one leased server are more challenging than those with two, as demonstrated by the gap between the lower and upper bounds, both between the linear relaxation bound and the best solution found and between the best final lower bound and the best solution. This feature is discussed further in \ref{app:artificial}.

As a final note, we point out that the objective function was structured to lexicographically first strive for feasibility and then optimize within the feasible domain. In fact, all instances have always resulted in feasible solutions. Reducing the cost coefficient of the request slack variables allows us to explore the trade-off between solution cost and the probability of reaching an infeasible solution. However, we did not explore this further.

\subsubsection*{Test case solution}

To further evaluate the benefits of the proposed approach, we compared the effectiveness of the robust, BESO-optimized solution with that obtained using a parametric method which assumed a Gaussian distribution of forecasts from the request datasets. This comparison was conducted using scenarios derived from a Gaussian distribution based on the confidence intervals of the forecasts provided by an Holt-Winters ETS model \citep{HKOS08}.

Figure \ref{fig:compDistr75} shows the distributions of the forecasts of the 75 boostset obtained by BESO and by ETS. Note that the BESO distribution, given its MEB foundation, spans a wider range of values than the ETS distribution.

\begin{figure*}[ht] 
	\centering
	\includegraphics[width=0.75\linewidth]{figs/compDistr.eps}%
	\caption{Comparison of distributions on 75 bootstrapped series}
	\label{fig:compDistr75}
\end{figure*}

The wider distribution has an impact on the proposed allocation. We had empirical proof of this when we tested the BESO and ETS solutions on the validation data obtained by the end of the year — i.e. when we finally received the data for which we were asked to provide a forecast.

Table \ref{table:compETS} presents a comparison of solutions in the first two columns when no rental cost was added for the use of leased floor space and in the last two columns when the rental cost was included in the objective function. As expected, given that the constraints were less restrictive, the ETS solution is less expensive in both cases. However, in both cases, the solution was infeasible as it did not provide all the goods requested by one of the retailers. This result was obtained in a single instance; therefore, it may not have happened. However, it is significant that, even in the first instance in which we tested our model, its robustness proved important in dealing with a limit case.

\begin{table}
	\centering
	\begin{tabular}{l|cc|cc|}
		& \multicolumn{2}{|c|}{No rental cost} & \multicolumn{2}{|c|}{With rental cost} \\
		& BESO  & ETS   & BESO    & ETS     \\
		\hline
		Assign. Cost     & 17781 & 15423 & 5446720 & 4378380 \\
		Optimality gap\% & 0     & 0     & 16.07   & 0.69    \\
		Infeasibilities  & 0     & 1     & 0       & 1       \\
		t.Cpu            & 33    & 29    & 3600    & 3600    \\
		\hline
	\end{tabular}
	\caption{Solution cost with and without rental cost}
	\label{table:compETS}
\end{table}

Table \ref{table:resETS} details the proposed usage of the  DCs in the different cases. It appears that DC 2, the biggest owned one, is the overloaded one in both cases, even though by small amounts. 

\begin{table}
	\centering
	\begin{tabular}{l|cc|cc|}
		& \multicolumn{2}{|c|}{No rental cost} & \multicolumn{2}{|c|}{With rental cost} \\
		& BESO  & ETS   & BESO    & ETS     \\
		\hline
		DC 0 & 67 / 120 & 119 / 120 & 82 / 120 & 116 / 120 \\
		DC 1 & 571 / 1000 & 430 / 1000 & 532 / 1000 & 428 / 1000 \\
		DC 2 & 257 / 300 & 301 / 300 & 257 / 300 & 303 / 300 \\
		DC 3 & 128 / 180 & 173 / 180 & 152 / 180 & 176 / 180 \\
		\hline
	\end{tabular}
	\caption{DC usage with and without rental cost}
	\label{table:resETS}
\end{table}

Finally, we note that Table \ref{table:compETS} shows that including the rental cost makes otherwise identical instances much harder to solve. In fact, the time limit of 3600 CPU seconds was too short to reach optimality, and a duality gap was left in the proposed solutions, a substantial one in the case of the BESO instance.

\section{Conclusions and future directions} \label{Sec:conclusions}

This paper introduces Bootstrap Enhanced Scenario Optimization (BESO), a framework for establishing predictive prescriptions when reliable data series are available. The method couples a statistically principled forecasting engine with scenario-based optimization by generating future scenarios through bootstrapped autoregressive time series, and explicitly accounts for feedback from the downstream optimization phase in the forecasting process. This is demonstrated through a two-echelon logistics case study, in which bootstrapped time series are used to forecast retailer requests and guide the allocation of stores to distribution centers (DCs). By capturing the empirical dynamics embedded in historical data, BESO supports the derivation of inventory requirements, DC capacity decisions and allocation strategies that reflect cost structures and forecast uncertainty.

A key contribution of BESO lies in offering a credible alternative to prevailing Distributionally Robust Optimization (DRO) methodologies. Classical DRO approaches rely on ambiguity sets defined through moment bounds, Wasserstein balls, or $\phi$-divergences, which—while theoretically elegant—impose distributional geometry that coul not match real empirical patterns, especially when the data exhibit nonstationarity, heavy tails, or structural breaks. In contrast, BESO avoids the explicit construction of ambiguity sets by directly generating empirical scenario distributions through the Maximum Entropy Bootstrap (MEB). Moreover, the framework allows for a lightweight objective-aware extension of MEB, in which elements derived from downstream optimization costs can be incorporated into the entropy maximization step, thereby biasing the resampling distribution toward patterns that are more related to decision quality. This produces scenario sets that are simultaneously statistically coherent, temporally realistic, and operationally aligned, thus offering a data-centric and computationally transparent alternative to established DRO formulations.

Through both real logistics data and controlled artificial instances, we show how BESO yields interpretable and robust prescriptions. The artificial benchmarks also revealed how the complexity of an instance depends on its different elements, outlining the fitness landscape and illustrating the framework’s adaptability to diverse uncertainty environments. These results suggest that integrating forecasting and optimization within a unified empirical loop can offer a solution stability and an operational performance that rivals classical scenario-generation or DRO-based approaches.

Several promising research directions emerge. Future work could explore incorporating MEB into preprocessing pipelines, such as Box–Cox transformations, to enhance predictive accuracy. Ensemble strategies, including weighted or Bayesian model averaging, may strengthen robustness in environments with shifting dynamics. On the methodological level, deeper investigation into bootstrap-driven ambiguity representations could position BESO as a data-implicit contribution to DRO, retaining robustness while avoiding restrictive geometric assumptions. Finally, the closure properties of the MEB framework could facilitate the development of hybrid schemes that integrate external knowledge or Bayesian priors, while further refining the incorporation of optimization-aware elements within the resampling process. Crucially, further work is required to extend BESO's applicability to environments where reliable time-series forecasting models cannot be established, thereby removing the current framework's reliance on predictable data dynamics.

%\clearpage % Ensures all pending floats are placed before this point

\appendix
\section{Bootstrap sets error costs} \label{app:errcost}

This section reports the absolute values of the various cost functions, which were calculated using the forecast errors obtained by averaging the corresponding errors for each of the 52 test case series described in the paper.

\begin{table}[h]
	\centering
	\renewcommand{\arraystretch}{0.8}
	\begin{tabular}{ccccccc}
		model & bias & MAPE & ME & MAE & MPE & RMSE \\
		\hline
		fcast\_50   &-0.27 & 0.09 &-1.02 & 1.63 &-0.05 &  2.12 \\
		fcast\_avg  & 0.02 & 0.07 & 0.07 & 1.25 & 0.01 &  1.68 \\
		yar         & 0.36 & 0.34 & 1.36 & 5.86 & 0.07 &  9.75 \\
		yhw         & 0.63 & 0.34 & 2.41 & 5.99 & 0.13 & 10.59 \\
		ysvm        & 0.69 & 0.36 & 2.66 & 6.20 & 0.14 & 10.99 \\
		ylstm       & 0.56 & 0.35 & 2.13 & 6.01 & 0.11 & 10.41 \\
		ymlp        & 0.47 & 0.36 & 1.79 & 6.26 & 0.09 & 10.60 \\
		yrf         & 1.50 & 0.44 & 5.73 & 7.79 & 0.31 & 13.54 \\
		yxgb        & 2.28 & 0.58 & 8.74 &10.12 & 0.49 & 17.05 \\
		yarima      & 0.44 & 0.34 & 1.67 & 5.80 & 0.09 &  9.77 \\
		\hline
	\end{tabular}
	\caption{Error costs for the 75 series boostset.}
	\label{table:app75}
\end{table}

\begin{table}[h]
	\centering
	\renewcommand{\arraystretch}{0.8}
	\begin{tabular}{ccccccc}
		model & bias & MAPE & ME & MAE & MPE & RMSE \\
		\hline
		fcast\_50   & -0.56 & 0.09 & -1.08 & 1.59 & -0.06 & 2.25  \\
		fcast\_avg  &  0.02 & 0.07 &  0.05 & 1.18 &  0.01 & 1.65  \\
		yar         & -0.59 & 0.25 & -1.13 & 4.36 & -0.07 & 6.31  \\
		yhw         & -0.13 & 0.24 & -0.26 & 4.33 & -0.02 & 6.39  \\
		ysvm        & -0.06 & 0.24 & -0.11 & 4.18 & -0.01 & 6.39  \\
		ylstm       & -0.30 & 0.24 & -0.58 & 4.15 & -0.04 & 6.34  \\
		ymlp        & -0.47 & 0.24 & -0.90 & 4.22 & -0.05 & 5.99  \\
		yrf         &  1.27 & 0.28 &  2.44 & 5.01 &  0.14 & 7.48  \\
		yxgb        &  2.52 & 0.38 &  4.85 & 6.52 &  0.28 & 9.46  \\
		yarima      & -0.40 & 0.25 & -0.76 & 4.44 & -0.05 & 6.83  \\
		\hline
	\end{tabular}
	\caption{Error costs for the 125 series boostset.}
	\label{table:app125}
\end{table}

\begin{table}[h]
	\centering
	\renewcommand{\arraystretch}{0.8}
	\begin{tabular}{ccccccc}
		model & bias & MAPE & ME & MAE & MPE & RMSE \\
		\hline
		fcast\_50  & -0.04 & 0.07 & -0.07 & 1.22 & 0     & 1.51 \\
		fcast\_avg & 0.99  & 0.12 & 1.90  & 2.04 & 0.12  & 2.76 \\
		yar        & -0.22 & 0.35 & -0.42 & 5.84 & -0.03 & 7.64 \\
		yhw        & 0.25  & 0.34 & 0.48  & 5.75 & 0.02  & 7.90 \\
		ysvm       & 0.30  & 0.33 & 0.58  & 5.66 & 0.03  & 7.76 \\
		ylstm      & 0.10  & 0.34 & 0.20  & 5.76 & 0     & 7.74 \\
		ymlp       & -0.19 & 0.34 & -0.36 & 5.69 & -0.02 & 7.57 \\
		yrf        & 1.72  & 0.37 & 3.31  & 6.39 & 0.18  & 9.66 \\
		yxgb       & 3.01  & 0.46 & 5.78  & 7.96 & 0.33  & 11.83\\
		yarima     & -0.04 & 0.34 & -0.08 & 5.69 & -0.01 & 7.63 \\
		\hline
	\end{tabular}
	\caption{Error costs for the 175 series boostset.}
	\label{table:app175}
\end{table}

The functions reported in the columns are: bias, Means Absolute Percentage Error (MAPE), Mean Error (ME), Mean Absolute Error (MAE), Mean Percentage Error (MPE) and Root Mean Square Error (RMSE). Table \ref{table:app75} gives the cost values for the case of a boost set composed of 75 series, table \ref{table:app125} for the 125 series and table \ref{table:app175} for the 175 series sets.

All tables show significantly consistent results across all boost set sizes.


\section{Extended deterministic benchmark set} \label{app:artificial}

All analyses reported in sections \ref{Subsec:predictive} and \ref{Subsec:prescriptive} refer to the case study at the heart of this paper, but the interest of the method is not limited to this particular case study as it can be effective on any instance of this 2-echelon supply chain problem. To test the generality of the results, we generated a benchmark set with varying dimensions and coefficients. The study was conducted only with respect to the deterministic setting, and is therefore significant for assessing the computational complexity of the split-allocation problem as a deterministic combinatorial optimization problem.

We generated 5 instances for each configuration of number of clients ($n$), number of servers ($m$), and number of splittable clients. The travel costs were obtained by randomly selecting $m$ rows and $n$ columns from a large distance matrix, whose values were computed as the actual road distances between points located in the same area as the test case. Server inventory costs were inversely proportional to the average travel cost from the server to the clients and client requests were inversely proportional to the average travel cost from the client to the servers. Note that the distance matrix is asymmetric, as it was computed within a city area. The number $n_b$ of splittable clients was progressively increased as the number $n_q$ of servers with nonzero inventory costs. Specifically, the range of the defining parameters were:
\begin{itemize}
	\item $n$: \{50, 52, 100, 200, 300\};
	\item $m$: \{4, 5, 10, 25, 50\};
	\item $n_b$: \{0, 4, 5, 10, 25, 50\}
	\item $n_q$: \{0, 1, 2, 3, 4, 5\}
\end{itemize}

Not all combinations of values were generated, the larger values were used only for instances with higher number of clients. In particular, the values 4 and 52 were only used to generate instances with the same dimensionality as the case study, in order to check the generality of the results obtained. The first instance of this set was kept to be the case study one.

The computational results in the case of no costly server, reported as averages over the 5 generated instances, are summarized in table \ref{table:extset}, whose columns show:
\begin{itemize}
	\item $n$, $m$: number of clients and number of servers;
	\item $n_b$: maximum number of splittable clients;
	\item $ncols$, $nrows$: number of columns and number of rows in formulation FD;
	\item $gaplb$: average percentage gap between the linear relaxation lower bound and the cost of the best solution found;
	\item $gapfinal$: average percentage gap between the best lower bound at the end of the search (smallest cost of an unexpanded node) and the cost of the best solution found;
	\item $ninf$: number of instances for which no feasible solution was found;
	\item $nopt$: number of instances solved to proven optimality;
	\item $tcpu$: average CPU time for solving the instance, time to optimality or 3600 if optimality could not be proven;
\end{itemize}

The table shows two trends. One, with respect to instance size, shows an expected increase in complexity. While instances the size of our test case are easily solved to optimality by modern solvers, increasing the size, as defined by the number of variables and the number of constraints, quickly leads to instances that could not be solved to optimality within the 3600-second time limit. 
The number of splittable clients has an important effect on the complexity of the instance. Instances of the same absolute size, but with more splittable clients, usually have a looser linear relaxation (LP) bound and have more difficulty reaching optimality, as can be seen by the decreasing number of instances out of the 5 from each group that could be solved to optimality.

\begin{table}
	\centering
	\renewcommand{\arraystretch}{0.8}
	\begin{tabular}{cccccccccc}
		$n$ & $m$ & $n_b$ & $ncols$ & $nrows$ & $gaplb$ & $gapfinal$ & $ninf$ & $nopt$ & $tcpu$\\
		\hline
		52 &  4 &  0 &   416 &   316 & 0.21 & 0.01 & 0 & 5 &    0  \\
		\hline
		50 &  5 &  0 &   500 &   355 & 0.39 & 0.00 & 0 & 5 &    3  \\
		50 &  5 &  5 &   500 &   355 & 0.40 & 0.00 & 0 & 5 &    4  \\
		50 & 10 &  0 &  1000 &   610 & 1.32 & 0.00 & 0 & 5 &  383  \\
		50 & 10 &  5 &  1000 &   610 & 1.34 & 0.00 & 0 & 5 &  378  \\
		50 & 10 & 10 &  1000 &   610 & 1.35 & 0.13 & 0 & 4 & 1150  \\
		\hline
		100 & 10 &  0 &  1000 &  1210 & 0.50 & 0.12 & 0 & 2 & 1920  \\
		100 & 10 &  5 &  1000 &  1210 & 0.54 & 0.05 & 0 & 3 & 1514  \\
		100 & 10 & 10 &  1000 &  1210 & 0.57 & 0.09 & 0 & 3 & 2063  \\
		100 & 50 &  0 & 10000 &  5250 & 5.23 & 3.06 & 0 & 0 & 3600  \\
		100 & 50 &  5 & 10000 &  5250 & 4.77 & 2.70 & 0 & 0 & 3600  \\
		100 & 50 & 10 & 10000 &  5250 & 4.55 & 2.42 & 0 & 0 & 3600  \\
		100 & 50 & 25 & 10000 &  5250 & 4.52 & 2.61 & 0 & 0 & 3600  \\
		\hline
		200 & 10 &  0 &  4000 &  2410 & 0.17 & 0.05 & 0 & 2 & 2369  \\
		200 & 10 &  5 &  4000 &  2410 & 0.17 & 0.09 & 0 & 1 & 2893  \\
		200 & 10 & 10 &  4000 &  2410 & 0.18 & 1.01 & 0 & 0 & 3600  \\
		200 & 50 &  0 & 20000 & 10450 & 1.66 & 1.26 & 0 & 0 & 3600  \\
		200 & 50 & 10 & 20000 & 10450 & 1.68 & 1.29 & 0 & 0 & 3600  \\
		200 & 50 & 25 & 20000 & 10450 & 1.71 & 1.37 & 0 & 0 & 3600  \\
		\hline
		300 & 10 &  0 &  6000 &  3610 & 0.09 & 0.03 & 0 & 3 & 1798 \\
		300 & 10 &  5 &  6000 &  3610 & 0.09 & 0.07 & 0 & 0 & 3600 \\
		300 & 10 & 10 &  6000 &  3610 & 1.02 & 0.47 & 0 & 0 & 3600 \\
		300 & 50 &  0 & 30000 & 15650 & 0.94 & 0.73 & 0 & 0 & 3600  \\
		300 & 50 & 10 & 30000 & 15650 & 0.93 & 0.76 & 0 & 0 & 3600  \\
		300 & 50 & 25 & 30000 & 15650 & 0.96 & 0.84 & 0 & 0 & 3600  \\
		\hline
	\end{tabular}
	\caption{Extended benchmark set, deterministic case, no costly server}
	\label{table:extset}
\end{table}

Note that the optimality gap of the LP bound is usually small, except for the $n$=100 and $m$=50 instances, but the search still struggles to close this gap. Interestingly, the LP bound gets tighter as $n$ increases, but this does not make the search any easier.
The least-cost solution when optimality was not proven was usually found within the first 600 seconds of CPU time. However, we note that designing efficient heuristics for this problem is beyond the scope of this research. We could have tweaked the solver configuration to favor heuristic search at the expense of efficiency in proving optimality, or designed custom heuristics, but the focus here is on assessing the fitness landscape when road network costs are involved.

Table \ref{table:extsetq} reports results about tests varying the number of costly servers, the columns show:
\begin{itemize}
	\item $n$, $m$: number of clients and number of servers;
	\item $ncols$, $nrows$: number of columns and number of rows in formulation FD;
	\item lb0, t0: linear relaxation bound and CPU time in the case of no costly servers;
	\item lb1, t1: linear relaxation bound and CPU time in the case of 1 costly server;
	\item lb2, t2: linear relaxation bound and CPU time in the case of 2 costly servers;
	\item lb3, t3: linear relaxation bound and CPU time in the case of 3 costly servers;
	\item lb4, t4: linear relaxation bound and CPU time in the case of 4 costly servers;
	\item lb5, t5: linear relaxation bound and CPU time in the case of 5 costly servers;
\end{itemize}

The table reports results on two sets of instances. The top 4 rows contain aggregate results on a set of instances, 5 for each configuration, where inventory costs are directly proportional to average travel distances, while the bottom 4 rows have inventory costs inversely proportional to average travel distances.

\begin{table}
	\addtolength{\tabcolsep}{-3pt}
	\footnotesize 
	\centering
	\renewcommand{\arraystretch}{1.2}
	\begin{tabular}{cccc|cc|cc|cc|cc|cc|cc}
		n & m & ncols & nrows & lb0 & t0 & lb1 & t1 & lb2 & t2 & lb3 & t3 & lb4 & t4 & lb5 & t5 \\
		\hline
		52 & 4  & 416 & 316 & 0.86 & 0.2 & 0.17 & 0.81 & 0.08 & 0.22 & 0.01 & 0.02 & 0.00 & 0.02 & - & - \\
		50 & 5  & 500 & 355 & 1.33 & 0.06 & 2.77 & 0.06 & 1.46 & 0.13 & 0.11 & 14.31 & 0.06 & 7 & 0.05 & 0.93 \\
		100 & 5 & 1000 & 705 & 0.41 & 0.24 & 0.09 & 6.39 & 0.06 & 3.2 & 0.01 & 0.06 & 0.02 & 4.8 & 0.02 & 2.71 \\
		200 & 5 & 2000 & 1405 & 0.14 & 1.43 & 0.01 & 1.79 & 0.01 & 0.25 & 0.01 & 0.17 & 0.01 & 0.11 & 0.01 & 7.84 \\
		\hline
		52 & 4  & 416 & 316 & 0.86 & 0.36 & 0.24 & 0.22 & 0.22 & 0.14 & 0.26 & 0.09 & 0.05 & 1.9 & - & - \\
		50 & 5  & 500 & 355 & 1.33 & 0.16 & 0.52 & 0.07 & 0.50 & 5.8 & 0.13 & 10.11 & 0.05 & 1.37 & 0.02 & 0.22 \\
		100 & 5 & 1000 & 705 & 0.41 & 0.33 & 0.19 & 8.58 & 0.04 & 6.14 & 0.02 & 0.41 & 0.02 & 0.58 & 0.02 & 3.78 \\
		200 & 5 & 2000 & 1405 & 0.14 & 1.55 & 0.07 & 39.07 & 0.04 & 9.44 & 0.01 & 0.12 & 0.02 & 4.48 & 0.01 & 6.66 \\
		\hline
	\end{tabular}
	\caption{Extended benchmark set, deterministic case, increasing number of costly servers}
	\label{table:extsetq}
\end{table}

The complexity of solving instances increases non-monotonically with the number of costly servers, for both increasing and decreasing inventory costs. While the 0-cost instances are comparatively easy to solve, the CPU time required to prove optimality increases with the addition of the first costly servers and then decreases as more servers are paid to store inventory. However, this is not true of the gap between the linear relaxation bound and the cost of the optimal solution, where the instances with no costly servers always have the largest gap.
As expected, the CPU time required to prove optimality increases with the dimension of the case, and the case with an inverse correlation between storage cost and travel cost is more difficult to solve than the case with a positive correlation.
	
%% Loading bibliography style file
%\bibliographystyle{model1-num-names}
\bibliographystyle{cas-model2-names} 
\bibliography{GDObiblio}

\end{document}

